\chapter[面向车载信息物理融合的质量-开销均衡优化关键技术]{面向车载信息物理融合的质量-开销均衡优化关键技术}

本章将研究面向车载信息物理融合的质量-开销均衡优化关键技术。
具体内容安排如下：
\ref{section 4-1} 节是本章的引言，介绍了车联网中车载信息物理融合系统的研究现状及存在的不足，同时阐述本章的主要贡献。
\ref{section 4-2} 节阐述了协同感知与V2I上传场景。
\ref{section 4-3} 节给出了系统模型的详细描述。
\ref{section 4-4} 节形式化定义了最大化VCPS质量并最小化VCPS开销的双目标优化问题。
\ref{section 4-5} 节设计了基于多目标的多智能体深度强化学习算法。
\ref{section 4-6} 节搭建了实验仿真模型并进行了性能验证。
\ref{section 4-7} 节对本章的研究工作进行总结。

\section{引言}\label{section 4-1}

最新的感知技术、无线通信和计算模式推动了现代新能源汽车和智能网联汽车的发展。现代汽车中装备了各种车载感知器，以增强车辆的环境感知能力 \cite{zhu2017overview}。另一方面，V2X通信\cite{chen2020a}的发展使车辆、路侧设备和云端之间的合作得以实现。同时，车载边缘计算\cite{dai2021edge}是一个很有前途的范式，可以实现计算密集型和延迟关键型的智能交通系统应用 \cite{zhao2022foundation}。这些进展都成为了开发车载信息物理融合系统的强大驱动力。具体来说，通过协同感知和上传，车联网中的物理实体，如车辆、行人和路侧设备等，可以在边缘节点上构建为相应的逻辑映射。

车载信息物理融合中的检测、预测、规划和控制技术被广泛研究。大量工作聚焦于检测技术，例如雨滴数量检测\cite{wang2021deep}和驾驶员疲劳检测\cite{chang2018design}。针对车辆状态预测方法，研究人员提出了混合速度曲线预测\cite{zhang2019a}、车辆跟踪\cite{iepure2021a}和加速预测\cite{zhang2020data}等。同时，部分研究工作提出了不同的调度方案，例如基于物理比率-K干扰模型的广播调度\cite{li2020cyber}和基于既定地图模型的路径规划\cite{lian2021cyber}。此外，部分研究集中在智能网联车辆的控制算法上，例如车辆加速控制\cite{lv2018driving}、交叉路口控制\cite{chang2021an}和电动汽车充电调度\cite{wi2013electric}。这些关于状态检测、轨迹预测、路径调度和车辆控制的研究促进了各种ITS应用的实施。然而，这些工作忽略了感知和上传开销，假设高质量可用信息可以在VEC中构建。少数研究考虑了VCPS中的信息质量，例如时效性\cite{liu2014temporal, dai2019temporal}和准确性\cite{rager2017scalability, yoon2021performance}，但上述研究都没有考虑通过协同感知和上传，在VCPS中实现高质量低成本的信息物理融合。

本章旨在通过车辆协同感知与上传，构建基于车载信息物理融合的逻辑视图，并进一步在最大化车载信息物理融合质量和最小化视图构建开销方面寻求最佳平衡。然而，实现这一目标面临着以下主要挑战。首先，车联网中的信息高度动态，因此考虑感知频率、排队延迟和传输时延的协同效应，以确保信息的新鲜度和时效性是至关重要的。其次，物理信息是具有时空相关性的，不同车辆在不同的时间或空间范围内感应到的信息可能存在冗余或不一致性。因此，具有不同感知能力的车辆有望以分布式方式合作，以提高感知和通信资源的利用率。再次，物理信息在分布、更新频率和模式方面存在异质性，这给构建高质量视图带来很大挑战。最后，高质量的视图构建需要更高的感知和通信资源开销，这也是一个需要考虑的关键因素。综上所述，通过协同感知和上传，实现面向车载边缘计算的高质量、低开销视图具有重要意义，但也具有一定的挑战性。

本章致力于研究车载信息物理融合系统的质量-开销均衡优化问题，并通过协同感知与上传实现高质量、低开销的视图建模。本章的主要贡献如下：第一，提出了协同感知与V2I上传场景，考虑视图的及时性和一致性，设计了车载信息物理融合质量指标，并考虑边缘视图构建过程中信息冗余度、感知开销和传输开销，设计了车载信息物理融合开销指标。进一步，提出了一个双目标优化问题，在最大化VCPS质量的同时最小化VCPS开销。第二，提出了基于多目标的多智能体深度强化学习算法(Multi-Agent Multi-Objective Deep Reinforcement Learning, MAMO）。具体地，在车辆和边缘节点中分别部署智能体，车辆动作空间包括感知决策、感知频率、上传优先级和传输功率分配，而边缘节点动作空间是V2I带宽分配策略。同时，设计了决斗评论家网络（Dueling Critic Network, DCN），其根据状态价值(State-Value, SV)和动作优势(Action-Advantage, AA）评估智能体动作。系统奖励是一个一维向量，其中包含VCPS质量和VCPS利润，并通过差分奖励信用分配得到车辆的个人奖励，进一步通过最小-最大归一化得到边缘节点的归一化奖励。第三，建立了基于现实世界车辆轨迹的仿真实验模型，并将MAMO与三种对比算法进行比较，包括随机分配、分布式深度确定性策略梯度\cite{barth2018distributed}，以及多智能体分布式深度确定性策略梯度。此外，本文设计了两个指标，即单位开销质量（Quality Per Unit Cost, QPUC）和单位质量利润（Proﬁt Per Unit Quality, PPUQ）用于定量衡量算法实现的均衡。仿真结果表明，与其他算法相比，MAMO在最大化QPUC和PPUQ方面更具优势。

\section{协同感知与 V2I 上传场景}\label{section 4-2}

本章节介绍了协同感知与V2I上传场景。如图\ref{fig 4-1}所示，车辆配备各种车载感知器，如超声波雷达、激光雷达、光学相机和毫米波雷达，可以对环境进行感知。通过车辆间协同地感知，可以获得异质信息，包括其他车辆、弱势道路参与者、停车场和路边基础设施的状态。这些信息可用于在边缘节点中建立视图模型，并进一步用于支撑各种ITS应用，如自动驾驶\cite{bai2022hybrid}、智慧路口控制系统\cite{hadjigeorgious2023real}，以及全息城市交通流管理\cite{wang2023city}。逻辑视图需要融合车联网中物理实体的不同模式信息，以更好地反映实时物理车辆环境，从而提高ITS的性能。然而，构建高质量的逻辑视图可能需要更高的感知频率、更多的信息上传量以及更高的能量消耗。

本系统的工作流程如下：首先，车辆感知并排队上传不同物理实体的实时状态。接着，边缘节点将V2I带宽分配给车辆，同时，车辆确定传输功率。物理实体的视图是基于从车辆收到的异质信息进行融合建立的。需要注意的是，在该系统中，异质信息是由车辆以不同的感知频率感应到的，因此上传时的新鲜度会不同。虽然增加感知频率可以提高新鲜度，但会增加排队延迟和能源消耗。此外，多个车辆可能感知到特定物理实体的信息，若由所有车辆上传，则可能会浪费通信资源。因此，为了提高资源利用率，需要有效而经济地分配通信资源。在此基础上，为了最大化面向车载边缘计算的视图的VCPS质量并最小化VCPS开销，必须量化衡量边缘节点构建的视图的质量和开销，并设计高效经济的协同感知和上传的调度机制。

\begin{figure}[h]
\centering
  \includegraphics[width=1\columnwidth]{Fig4-1-architerture.pdf}
  \bicaption{协同感知与 V2I 上传场景}{Cooperative sensing and V2I uploading scenario}
  \label{fig 4-1}
\end{figure} 

\section{车载信息物理融合质量/开销模型}\label{section 4-3}
\subsection{基本符号}
本系统离散时间片的集合用$\mathbf{T}=\left\{1,\ldots,t,\ldots, T \right\}$表示。
异质信息集合用$\mathbf{D}$表示，其中信息$d \in \mathbf{D}$的特征是一个三元组$d=\left(\operatorname{type}_d, u_d, \left|d\right| \right)$，其中$\operatorname{type}_d$、$u_d$和$\left|d\right|$分别是信息类型、更新间隔和数据大小。
$\mathbf{V}$表示车辆的集合，每个车辆$v\in \mathbf{V}$的特征是一个三元组$v=\left (l_v^t, \mathbf{D}_v, \pi_v \right )$，其中$l_v^t$、$\mathbf{D}_v$和$\pi_v$分别是位置、感知的信息集和传输功率。
对于$d \in \mathbf{D}_v$，车辆$v$的感知开销（即能耗）用$\phi_{d, v}$表示。
用$\mathbf{E}$表示边缘节点的集合，其中每个边缘节点$e \in \mathbf{E}$的特征是$e=\left (l_e, g_e, b_e \right)$，其中$l_{e}$、$r_{e}$和$b_{e}$分别为位置、通信范围和带宽。
车辆$v$与边缘节点$e$之间的距离表示为$\operatorname{dis}_{v, e}^t \triangleq \operatorname{distance} \left (l_v^t, l_e \right ), \forall v \in \mathbf{V}, \forall e \in \mathbf{E}, \forall t \in \mathbf{T}$。
在时间$t$内处于边缘节点$e$的通信覆盖范围内的车辆集合表示为$\mathbf{V}_e^t=\left \{v \vert \operatorname{dis}_{v, e}^t \leq g_e, \forall v \in \mathbf{V} \right \}, \mathbf{V}_e^t \subseteq \mathbf{V}$。

感知决策指示器表示车辆$v$在时间$t$是否感知信息$d$，其用以下方式表示：
\begin{equation}
	c_{d, v}^t \in \{0, 1\}, \forall d \in \mathbf{D}_{v}, \forall v \in \mathbf{V}, \forall t \in \mathbf{T}
	\label{equ 4-1} 
\end{equation}
那么，车辆$v$在时间$t$的感应信息集合表示为 $\mathbf{D}_v^t = \{ d | c_{d, v}^{t} = 1, \forall d \in \mathbf{D}_v \}, \mathbf{D}_v^t \subseteq \mathbf{D}_v$。
对于任何信息$d \in \mathbf{D}_v^t$来说，信息类型都是不同的, 即$\operatorname{type}_{d^*} \neq \operatorname{type}_{d}, \forall d^* \in \mathbf{D}_v^t \setminus \left\{ d\right \}, \forall d \in \mathbf{D}_v^t$。
车辆$v$在时间$t$的信息$d$的感知频率用$\lambda_{d, v}^t$表示，其需要满足车辆$v$的感应能力要求。
\begin{equation}
	\lambda_{d, v}^{t} \in [\lambda_{d, v}^{\min} , \lambda_{d, v}^{\max} ], \ \forall d \in \mathbf{D}_v^t, \forall v \in \mathbf{V}, \forall t \in \mathbf{T}
\end{equation}
其中$\lambda_{d, v}^{\min}$和$\lambda_{d, v}^{\max}$分别是车辆$v$中信息${d}$的最小和最大感知频率。
车辆$v$中的信息$d$在时间$t$的上传优先级用$p_{d, v}^t$表示，不同信息的上传优先级需各不相同。
\begin{equation}
	{p}_{d^*, v}^t \neq {p}_{d, v}^t, \forall d^* \in \mathbf{D}_v^t \setminus \left\{ d\right \}, \forall d \in \mathbf{D}_v^t, \forall v \in \mathbf{V}, \forall t \in \mathbf{T}
\end{equation}
其中${p}_{d^*, v}^t$是信息$d^* \in \mathbf{D}_v^t$中的上传优先级。
车辆$v$在时间$t$的传输功率用$\pi_{v}^t$表示，其不能超过车辆$v$的功率容量。
\begin{equation}
	\pi_v^t \in \left[ 0 , \pi_v \right ], \forall v \in \mathbf{V}, \forall t \in \mathbf{T}
\end{equation}
边缘节点$e$在时间$t$为车辆$v$分配的V2I带宽用$b_{v, e}^t$表示，且其需要满足：
\begin{equation}
	b_{v, e}^t \in \left [0, b_e \right], \forall v \in \mathbf{V}_e^{t}, \forall e \in \mathbf{E}, \forall t \in \mathbf{T}
	\label{equ 4-5} 
\end{equation}
边缘节点$e$分配的V2I总带宽不能超过其容量$b_e$，即${\sum_{\forall v \in \mathbf{V}_e^{t}} b_{v, e}^t} \leq b_e, \forall t \in \mathbf{T}$。

本系统中物理实体的集合为 $\mathbf{I}^{\prime}$，其中$i^{\prime} \in \mathbf{I}^{\prime}$表示物理实体，如车辆、行人和路侧基础设施等。
$\mathbf{D}_{i^{\prime}}$是与实体$i^{\prime}$相关的信息集合，可以用$\mathbf{D}_{i^{\prime}}=\left\{d \mid y_{d, i^{\prime}} = 1, \forall d \in \mathbf{D} \right\}$, $\forall i^{\prime} \in \mathbf{I}^{\prime}$表示, 其中$y_{d, i^{\prime}}$是一个二进制数，表示信息$d$是否与实体$i^{\prime}$关联。
$\mathbf{D}_{i^{\prime}}$的大小用$|\mathbf{D}_{i^{\prime}}|$表示。
每个实体可能需要多个信息，即$|\mathbf{D}_{i^{\prime}}| = \sum_{\forall d \in \mathbf{D}}y_{d, i^{\prime}} \geq 1, \forall i^{\prime} \in \mathbf{I}^{\prime}$。
对于每个实体$i^{\prime} \in \mathbf{I}^{\prime}$，可能有一个视图$i$在边缘节点中建模。
用$\mathbf{I}$表示视图的集合，用$\mathbf{I}_e^{t}$表示时间为$t$时在边缘节点$e$中建模的视图集合。
因此，边缘节点$e$收到且被视图$i$需要的信息集合可以用$\mathbf{D}_{i, e}^t=\bigcup_{\forall v \in \mathbf{V}}\left(\mathbf{D}_{i^{\prime}} \cap \mathbf{D}_{v, e}^t\right), \forall i \in \mathbf{I}_e^{t}, \forall e \in \mathbf{E}$表示，且 $| \mathbf{D}_{i, e}^t |$是边缘节点$e$收到且被视图$i$需要的信息数量，其计算公式为$| \mathbf{D}_{i, e}^t | =  \sum_{\forall v \in \mathbf{V}} \sum_{\forall d \in \mathbf{D}_v} c_{d, v}^t  y_{d, i^{\prime}}$。

\subsection{协同感知模型}
车辆协同感知是基于多类M/G/1优先级队列\cite{moltafet2020age}进行建模。
假设具有$\operatorname{type}_d$的信息的上传时间$\operatorname{\hat{g}}_{d, v, e}^t$遵循均值$\alpha_{d, v}^t$和方差$\beta_{d, v}^t$的一类一般分布。
那么，车辆$v$中的上传负载$\rho_{v}^{t}$由$ \rho_{v}^{t}=\sum_{\forall d \subseteq \mathbf{D}_v^t} \lambda_{d, v}^{t} \alpha_{d, v}^t$表示。
根据多类M/G/1优先级队列，需要满足$\rho_{v}^{t} < 1$才能达到队列的稳定状态。
信息$d$在时间$t$之前的到达时间用$\operatorname{a}_{d, v}^t$表示，其计算公式为：
\begin{equation}
    \operatorname{a}_{d, v}^t =  \frac{\left \lfloor t \lambda_{d, v}^t \right \rfloor }{\lambda_{d, v}^{t}} 
\end{equation}
在时间$t$之前，由$\operatorname{u}_{d, v}^t$表示的信息$d$的更新时间是通过下式计算：
\begin{equation}
    \operatorname{u}_{d, v}^t = \left \lfloor  \frac{\operatorname{a}_{d, v}^t}{u_d} \right \rfloor  u_d
\end{equation}
其中$u_d$是信息$d$的更新间隔时间。


在时间$t$，车辆$v$中比$d$有更高上传优先级的信息集合，用$\mathbf{D}_{d, v}^t = \{ d^* \mid p_{d^*, v}^{t} > p_{d, v}^{t} , \forall d^* \in \mathbf{D}_v^t \}$表示，其中$p_{d^*, v}^{t}$是信息$d^* \in \mathbf{D}_v^t$的上传优先级。
因此，信息$d$前面的上传负载（即$v$在时间$t$时要在$d$之前上传的信息数量）通过下方计算得出： 
\begin{equation}
	\rho_{d, v}^{t}=\sum_{\forall d^* \in \mathbf{D}_{d, v}^t} \lambda_{d^*, v}^t \alpha_{d^*, v}^t
\end{equation}
其中$\lambda_{d^*, v}^t$和$\alpha_{d^*, v}^t$分别为时间$t$内车辆$v$中信息$d^*$的感知频率和平均传输时间。
根据Pollaczek-Khintchine公式\cite{takine2001queue}，车辆$v$中信息$d$的排队时间计算如下：
\begin{equation}
    \operatorname{q}_{d, v}^t= \frac{1} {1 - \rho_{d, v}^{t}} 
        \left[ \alpha_{d, v}^t + \frac{ \lambda_{d, v}^{t} \beta_{d, v}^t + \sum\limits_{\forall d^* \in \mathbf{D}_{d, v}^t} \lambda_{d^*, v}^t \beta_{d^*, v}^t }{2\left(1-\rho_{d, v}^{t} - \lambda_{d, v}^{t} \alpha_{d, v}^t\right)}\right] 
        - \alpha_{d, v}^t
\end{equation}

\subsection{V2I协同上传模型}
车辆间V2I协同上传是基于信道衰减分布和信噪比阈值来建模的。
车辆$v$和边缘节点$e$之间的V2I通信在时间$t$的信噪比通过公式\ref{equ 4-10}\cite{sadek2009distributed}计算得到。
\begin{equation}
    \operatorname{SNR}_{v, e}^{t}=\frac{1}{N_{0}} \left|h_{v, e}\right|^{2} \tau {\operatorname{dis}_{v, e}^{t}}^{-\varphi} {\pi}_v^t
    \label{equ 4-10}
\end{equation}
其中$N_{0}$为AWGN；$h_{v, e}$为信道衰减增益；$\tau$为取决于天线设计的常数；$\varphi$为路径损耗指数。
假设$\left|h_{v, e}\right|^{2}$遵循均值$\mu_{v, e}$和方差$\sigma_{v, e}$的一类分布，其表示方法为：
\begin{equation}
    \tilde{p}=\left\{\mathbb{P}: \mathbb{E}_{\mathbb{P}}\left[\left|h_{v, e}\right|^{2}\right]=\mu_{v, e}, \mathbb{E}_{\mathbb{P}}\left[\left|h_{v, e}\right|^{2}-\mu_{v, e}\right]^{2}=\sigma_{v, e}\right\}
\end{equation}
进一步，基于成功传输概率和可靠性阈值来衡量V2I传输可靠性。
\begin{equation}
    \inf_{\mathbb{P} \in \tilde{p}} \operatorname{Pr}_{[\mathbb{P}]}\left(\operatorname{SNR}_{v, e}^{t} \geq \operatorname{SNR}_{v, e}^{\operatorname{tgt}}\right) \geq \delta
\end{equation}
\noindent 其中$\operatorname{SNR}_{v, e}^{\operatorname{tgt}}$和$\delta$分别为目标SNR阈值和可靠性阈值。
由车辆$v$上传并由边缘节点$e$接收的信息集合用$\mathbf{D}_{v, e}^{t} = \bigcup_{\forall v \in \mathbf{V}_{e}^{t}} \mathbf{D}_{v}^{t}$表示。

根据香农理论，车辆$v$和边缘节点$e$之间在时间$t$的V2I通信的传输率用$\operatorname{z}_{v, e}^t$表示，其计算公式如下：
\begin{equation}
    \operatorname{z}_{v, e}^t=b_{v}^{t} \log _{2}\left(1+\mathrm{SNR}_{v, e}^{t}\right)
\end{equation}
假设车辆$v$被安排在时间$t$上传$d$，并且$d$将在一定的排队时间$\mathrm{\bar{q}}_{d, v}^t$后被传输。
然后，本章把车辆$v$开始传输$d$的时刻表示为$\mathrm{t}_{d, v}^t=t+\mathrm{q}_{d, v}^t$。
从$\mathrm{t}_{d, v}^t$到$\mathrm{t}_{d, v}^t + f$之间传输的数据量可由 $\int_{\mathrm{t}_{d, v}^t}^{\mathrm{t}_{d, v}^t+f} \mathrm{z}_{v, e}^t \mathrm{~d} t$ bits 得到，其中$f \in \mathbb{R}^{+}$和$\mathrm{z}_{i, e}^t$是时间$t$的传输速率。
如果在整个传输过程中可以传输的数据量大于信息$d$的大小，那么上传就会完成。
因此，从车辆$v$到边缘节点$e$传输信息$d$的时间，用$\operatorname{g}_{d, v, e}^t$表示，计算如下：
\begin{equation}
    \operatorname{g}_{d, v, e}^t=\inf _{j \in \mathbb{R}^+} \left \{ \int_{\operatorname{k}_{d, v}^t}^{\operatorname{k}_{d, v}^t + j} {\operatorname{z}_{v, e}^t} \operatorname{d}t \geq \left|d\right| \right \} 
\end{equation}
\noindent 其中$\operatorname{t}_{d, v}^t = t +\operatorname{q}_{d, v}^t$是车辆$v$开始传输信息$d$的时刻。

\section{质量-开销均衡问题定义}\label{section 4-4}

\subsection{VCPS质量}
首先，由于视图是基于连续上传和时间变化的信息建模的，本章对信息$d$的及时性定义如下：
\begin{definition}
信息$d$在车辆$v$中的及时性$\theta_{d, v} \in \mathbb{Q}^{+}$被定义为更新和接收信息$d$之间的时间差。
\begin{equation}
    \theta_{d, v} = \operatorname{a}_{d, v}^t + \operatorname{q}_{d, v}^t + \operatorname{g}_{d, v, e}^t-\operatorname{u}_{d, v}^{t}, \forall d \in \mathbf{D}_v^t,\forall v \in \mathbf{V}
\end{equation}
\end{definition}
\begin{definition}
视图$i$的及时性 $\Theta_{i} \in \mathbb{Q}^{+}$定义为与物理实体$i^{\prime}$相关的信息的最大及时性之和。
	\begin{equation}
    	\Theta_{i} = \sum_{\forall v\in \mathbf{V}_{e}^{t}} \max_{\forall d \in \mathbf{D}_{i^{\prime}} \cap \mathbf{D}_v^t}\theta_{d, v}, \forall i \in \mathbf{I}_{e}^{t}, \forall e \in \mathbf{E}
    	\label{equ 4-16}
	\end{equation}
\end{definition}

其次，由于不同类型的信息有不同的感知频率和上传优先级，本章定义视图的一致性来衡量与同一物理实体相关的信息的一致性。
\begin{definition}
视图$i$的一致性$\Psi_{i} \in \mathbb{Q}^{+}$定义为信息更新时间差的最大值。
\begin{equation}
    \Psi_{i}=\max_{\forall d \in \mathbf{D}_{i, e}^{t}, \forall v \in \mathbf{V}_{e}^{t}} {\operatorname{u}_{d, v}^t} - \min_{\forall d \in \mathbf{D}_{i, e}^{t}, \forall v \in \mathbf{V}_{e}^{t}} {\operatorname{u}_{d, v}^t} , \forall i \in \mathbf{I}_{e}^{t}, \forall e \in \mathbf{E}
\end{equation}
\end{definition}

最后，本章给出了视图的质量的正式定义，其综合了视图的及时性和一致性。
\begin{definition}
视图质量$\operatorname{QV}_{i} \in (0, 1)$定义为视图$i$的归一化及时性和归一化一致性的加权平均和。
	\begin{equation}
	    \operatorname{QV}_{i} = w_1 (1 -\hat{\Theta_{i}}) + w_2 (1 - \hat{\Psi_{i}}), \forall i \in \mathbf{I}_{e}^t, \forall e \in \mathbf{E}
	\end{equation}
\end{definition}
\noindent 其中$\hat{\Theta_{i}} \in (0, 1)$和$\hat{\Psi_{i}} \in (0, 1)$分别表示归一化的及时性和归一化的一致性，这可以通过最小-最大归一化对及时性和一致性的范围进行重新调整至$(0, 1)$来获得。
$\hat{\Theta_{i}}$和$\hat{\Psi_{i}}$的加权系数分别用$w_1$和$w_2$表示，可以根据ITS应用的不同要求进行相应的调整，$w_1+w_2=1$。
进一步，基于视图质量定义车载信息物理融合质量如下：
\begin{definition}
VCPS质量$\mathscr{Q} \in (0, 1)$被定义为在调度期间$\mathbf{T}$的边缘节点中建模的每个视图的QV的平均值。
	\begin{equation}
		\mathscr{Q}=\frac{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} \sum_{\forall i \in \mathbf{I}_e^t} \operatorname{QV}_{i}}{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} |\mathbf{I}_e^t| }
	\end{equation}
\end{definition}

\subsection{VCPS开销}

首先，由于同一物理实体的状态可能被多个车辆同时感应到，本章对信息$d$的冗余度定义如下：
\begin{definition}
信息$d$的冗余度$\xi_d \in \mathbb{N}$定义为车辆感应到同一类型$\operatorname{type}_d$的额外信息数量。
\begin{equation}
    \xi_d= \left | \mathbf{D}_{d, i, e} \right| - 1, \forall d \in \mathbf{D}_j, \forall i \in \mathbf{I}_{e}^{t}, \forall e \in \mathbf{E}
\end{equation}
\noindent 其中$\mathbf{D}_{d, i, e}$是边缘节点$e$收到且被视图$i$需要，且类型为$\operatorname{type}_d$的信息集合，其由$\mathbf{D}_{d, i, e}=\left\{ d^* \vert \operatorname{type}_{d^*} = \operatorname{type}_{d}, \forall d^* \in \mathbf{D}_{i, e}^t \right \}$表示。

\end{definition}
\begin{definition}
视图$i$的冗余度$\Xi_j \in \mathbb{N}$定义为视图$i$中的总冗余度。
	\begin{equation}
       \Xi_j =  \sum_{\forall d \in \mathbf{D}_{i^{\prime}}} \xi_d, \forall i \in \mathbf{I}_{e}^{t}, \forall e \in \mathbf{E}
       \label{equ 4-20}
    \end{equation}
\end{definition}

其次，信息感知和传输需要消耗车辆的能量，本章定义视图$i$的感知开销和传输开销如下：
\begin{definition}
视图$i$的感知开销$\Phi_{i} \in \mathbb{Q}^{+}$定义为视图$i$所需信息的总感知开销。
	\begin{equation}
        \Phi_{i} = \sum_{\forall v \in \mathbf{V}_{e}^{t}} \sum_{\forall d \in \mathbf{D}_{i^{\prime}} \cap \mathbf{D}_v^t}{\phi_{d, v}}, \forall i \in \mathbf{I}_{e}^t, \forall e \in \mathbf{E}
        \label{equ 4-21}
    \end{equation}
    其中$\phi_{d, v}$是信息$d$在车辆$v$中的感知开销。
\end{definition}
\begin{definition}
信息$d$在车辆$v$中的传输开销${\omega}_{d, v} \in \mathbb{Q}^{+}$定义为信息上传时消耗的传输功率。
\begin{equation}
    {\omega}_{d, v}= \pi_v^t \operatorname{g}_{d, v, e}^t, \forall d \in \mathbf{D}_v^t
\end{equation}
其中$\pi_v^t$和$\operatorname{g}_{d, v, e}^t$分别为传输功率和传输时间。
\end{definition}
\begin{definition}
视图$i$的传输开销$\Omega_{i} \in \mathbb{Q}^{+}$定义为视图$i$所需的信息总传输开销。
	\begin{equation}
        \Omega_{i} = \sum_{\forall v \in \mathbf{V}_{e}^{t}} \sum_{\forall d \in \mathbf{D}_{i^{\prime}} \cap \mathbf{D}_v^t} {\omega}_{d, v}, \forall i \in \mathbf{I}_{e}^t, \forall e \in \mathbf{E}
       	\label{equ 4-23}
    \end{equation}
\end{definition}

最后，给出视图开销的正式定义，其综合了冗余度、感知开销和传输开销。
\begin{definition}
视图的开销$\operatorname{CV}_{i} \in (0, 1)$定义为视图$i$的归一化冗余度、归一化感知开销和归一化传输开销的加权平均和。
	\begin{equation}
	    \operatorname{CV}_{i} = w_3  \hat{\Xi_{i}} +  w_4 \hat{\Phi_{i}} + w_5 \hat{\Omega_{i}}, \forall i \in \mathbf{I}_{e}^t, \forall e \in \mathbf{E}
	\end{equation}
\end{definition}
\noindent 其中 $\hat{\Xi_{i}}\in (0, 1)$、$\hat{\Phi_{i}} \in (0, 1)$和$\hat{\Omega_{i}} \in (0, 1)$ 分别表示视图$i$的归一化冗余度、归一化感知开销和归一化传输开销。
$\hat{\Xi_{i}}$、$\hat{\Phi_{i}}$和$\hat{\Omega_{i}}$ 的加权系数分别表示为 $w_3$、$w_4$和 $w_5$。
同样地，$w_3+w_4+w_5=1$。
进一步，VCPS开销定义如下：
\begin{definition}
VCPS 开销$\mathscr{C} \in (0, 1)$定义为$\mathbf{T}$调度期间边缘节点中每个视图模型的CV的平均值。
	\begin{equation}
		\mathscr{C}=\frac{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} \sum_{\forall i \in \mathbf{I}_e^t}  \operatorname{CV}_{i}}{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} |\mathbf{I}_e^t| }
	\end{equation}
\end{definition}

\subsection{双目标优化问题}
给定一个确定的解决方案$( \mathbf{C}, \bf\Lambda, \mathbf{P}, \bf\Pi, \mathbf{B} )$，其中$\mathbf{C}$表示确定的感知信息决策，$\bf\Lambda$表示确定的感知频率。$\mathbf{P}$表示确定的上传优先级，$\bf\Pi$表示确定的传输功率，$\mathbf{B}$表示确定的V2I带宽分配。
\begin{numcases}{}
	\mathbf{C} = \left \{ c_{d, v}^t \vert \forall d \in \mathbf{D}_{v}, \forall v \in \mathbf{V}, \forall t \in \mathbf{T} \right  \} \notag \\
	{\bf\Lambda} = \left \{ \lambda_{d, v}^{t} \vert \forall d \in \mathbf{D}_v^t  , \forall v \in \mathbf{V}, \forall t \in \mathbf{T} \right \} \notag \\ 
	\mathbf{P} = \left \{ p_{d, v}^{t} \vert \forall d \in \mathbf{D}_v^t  , \forall v \in \mathbf{V}, \forall t \in \mathbf{T}\right \}  \notag \\
	{\bf\Pi} = \left \{ \pi_v^t \vert \forall v \in \mathbf{V}, \forall t \in \mathbf{T} \right \} \notag \\
	\mathbf{B} = \left \{ b_v^t \vert \forall v \in \mathbf{V}, \forall t \in \mathbf{T}\right \}
\end{numcases}
其中$c_{d, v}^t$、$\lambda_{d, v}^{t}$和$p_{d, v}^{t}$分别为时间$t$内车辆$v$的信息$d$的感知信息决策、感知频率和上传优先级，$\pi_v^t$和$b_v^t$分别为时间$t$内车辆$v$的传输功率和V2I带宽。
本章提出了一个双目标优化问题，旨在同时实现VCPS质量的最大化和VCPS 开销的最小化，该问题的形式化定义如下：
\begin{align}
	\mathcal{P}4.1: & \max_{\mathbf{C}, \bf\Lambda, \mathbf{P}, \bf\Pi, \mathbf{B}} \mathscr{Q}, \min_{\mathbf{C}, \bf\Lambda, \mathbf{P}, \bf\Pi, \mathbf{B}} \mathscr{C} \notag \\
	\text { s.t. }
	& (\ref{equ 4-1}) \sim (\ref{equ 4-5}) \notag \\
    &\mathcal{C}4.1: \sum_{\forall d \subseteq \mathbf{D}_v^t} \lambda_{d, v}^{t} \mu_d<1,\ \forall v \in \mathbf{V}, \forall t \in \mathbf{T} \notag \\
    &\mathcal{C}4.2: \inf_{\mathbb{P} \in \tilde{p}} \operatorname{Pr}_{[\mathbb{P}]}\left(\operatorname{SNR}_{v, e}^{t} \geq \operatorname{SNR}_{v, e}^{\operatorname{tgt}}\right) \geq \delta, \forall v \in \mathbf{V}, \forall t \in \mathbf{T} \notag \\
    &\mathcal{C}4.3: {\sum_{\forall v \in \mathbf{V}_e^{t}}b_v^t} \leq b_e, \forall t \in \mathbf{T}
\end{align}
其中$\mathcal{C}4.1$保证队列稳定状态，$\mathcal{C}4.2$保证传输可靠性。
$\mathcal{C}4.3$要求边缘节点$e$分配的V2I带宽之和不能超过其容量$b_e$。
基于CV的定义，视图的利润定义如下：
\begin{definition}
视图的利润$\operatorname{PV}_{j} \in (0, 1)$定义为视图$i$的CV的补集。
	\begin{equation}
		\mathscr{P}= 1 - \operatorname{CV}_{i}
	\end{equation}
\end{definition}
\noindent 然后，本章将VCPS 利润定义如下：
\begin{definition}
VCPS 利润$\mathscr{P} \in (0, 1)$被定义为在调度期$\mathbf{T}$期间，边缘节点中每个视图模型的PV的平均值。
	\begin{equation}
		\mathscr{P}= \frac{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} \sum_{\forall i \in \mathbf{I}_e^t}   \operatorname{PV}_{j} }{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} |\mathbf{I}_e^t| }
	\end{equation}
\end{definition}
\noindent 因此，$\mathcal{P}4.1$问题可以改写如下：
\begin{align}
	\mathcal{P}4.2: & \max_{ \mathbf{C}, \bf\Lambda, \mathbf{P}, \bf\Pi, \mathbf{B} } \left (\mathscr{Q}, \mathscr{P} \right ) \notag \\
		\text { s.t. }
	&(\ref{equ 4-1}) \sim (\ref{equ 4-5}), \mathcal{C}4.1 \sim \mathcal{C}4.3
\end{align}

\section{基于多目标的多智能体强化学习算法设计}\label{section 4-5}

本章节提出了基于多目标的多智能体深度强化学习算法，其模型如图\ref{fig 4-2}所示，由$K$分布式行动者、学习器和经验回放缓存组成。
具体地，学习器由四个神经网络组成，即本地策略网络、本地评论家网络、目标策略网络和目标评论家网络。
其中车辆的本地策略网络、本地评论家网络、目标策略网络和目标评论家网络参数分别表示为 $\theta_{\mathbf{V}}^{\mu}$、$\theta_{\mathbf{V}}^{Q}$、 $\theta_{\mathbf{V}}^{\mu^{\prime}}$和$\theta_{\mathbf{V}}^{Q^{\prime}}$。
同样地，边缘节点的本地策略网络、本地评论家网络、目标策略网络和目标评论家网络参数分别表示为 $\theta_{\mathbf{E}}^{\mu}$、$\theta_{\mathbf{E}}^{Q}$、$\theta_{\mathbf{E}}^{\mu^{\prime}}$和$\theta_{\mathbf{E}}^{Q^{\prime}}$。
本地策略和本地评论家网络的参数是随机初始化的。
目标策略和目标评论家网络的参数被初始化为相应的本地网络。
然后，启动$K$分布式行动者，每个分布式行动者独立地与环境进行交互，并将交互经验存储到重放经验缓存。
分布式行动者由本地车辆策略网络和本地边缘策略网络组成，其分别用$\theta_{\mathbf{V}, k}^{\mu}$和$\theta_{\mathbf{E}, k}^{\mu}$表示，其网络参数是从学习器的本地策略网络复制而来的。
同时，初始化了最大存储容量为$|\mathcal{B}|$的经验回放缓存以存储重放经验。
基于多目标的多智能体深度强化学习的具体步骤见算法4.1，分布式行动者与环境的交互将持续到学习器的训练过程结束，其具体步骤见算法4.2。

\begin{figure}[h]
\centering
  \includegraphics[width=1\columnwidth]{Fig4-2-solution-model.pdf}
  \bicaption{基于多目标的多智能体深度强化学习模型}{Multi-agent multi-objective deep reinforcement learning model}
  \label{fig 4-2}
\end{figure}


\SetKwInOut{KwIn}{输入}
\SetKwInOut{KwOut}{输出}

\begin{algorithm}[h]\small
\renewcommand{\algorithmcfname}{算法}
	\caption{基于多目标的多智能体深度强化学习}
	\KwIn{折扣因子 $\gamma$、批大小 $M$、回放经验缓存 $\mathcal{B}$、学习率$\alpha$和$\beta$、目标网络参数更新周期 $t_{\operatorname{tgt}}$、分布式行动者网络参数更新周期 $t_{\operatorname{act}}$、随机行动数量$N$}
	\KwOut{信息感知决策$\mathbf{C}_v^t$、信息感知频率决策$\lambda_{d, v}^{t}$、上传优先级决策$p_{d, v}^{t}$、传输功率$\pi_v^t$、V2I带宽分配$b_{v, e}^{t}$}
	初始化网络参数\\
	初始化经验回放缓存 $\mathcal{B}$\\
	启动 $K$ 分布式行动者并复制网络参数给行动者\\
	\For{\songti{迭代次数} $= 1$ \songti{到最大迭代次数}}{
		\For{\songti{时间片} $t = 1$ \songti{到} $T$}{
			从经验回放缓存$\mathcal{B}$随机采样$M$小批量\\
			通过目标评论家网络中DCN网络得到目标值\\
			基于分类分布的TD学习计算更新评论家网络\\
			更新本地策略和评论家网络\\
			\If{$t \mod t_{\operatorname{tgt}} = 0$}{
				更新目标网络\\
			}
			\If{$t \mod t_{\operatorname{act}} = 0$}{
				复制网络参数给分布式行动者\\
			}
		}
	}
\label{algorithm 4-1}
\end{algorithm}

\begin{algorithm}[t]\small
\renewcommand{\algorithmcfname}{算法}
	\caption{分布式行动者}
	\KwIn{车辆探索常数 $\epsilon_{v}$、边缘节点探索常数$\epsilon_{e}$、车辆本地观测 $\boldsymbol{o}_{v}^{t}$、边缘节点本地观测 $\boldsymbol{o}_{e}^{t}$}
	\KwOut{车辆动作$\boldsymbol{a}_{\mathbf{V}}^{t}$、边缘节点动作$\boldsymbol{a}_{e}^{t}$}
	\While{\songti{学习器没有结束}}{
		初始化一个随机过程 $\mathcal{N}$ 以进行探索\\
		生成随机权重 $\boldsymbol{w}^{t}$\\
		接收初始系统状态 $\boldsymbol{o}_{1}$\\
		\For{\songti{时间片} $t = 1$ \songti{到} $T$}{
			\For{\songti{车辆} $v=1$ \songti{到} $V$}{
				接收车辆本地观测 $\boldsymbol{o}_{v}^{t}$\\
				选择一个车辆动作 $\mu_{\mathbf{V}}\left(\boldsymbol{o}_{v}^{t} \mid \theta_{\mathbf{V}}^{\mu}\right)+\epsilon_{v} \mathcal{N}_{v}^{t}$\\
			}
			接收边缘节点本地观测 $\boldsymbol{o}_{e}^{t}$\\
			选择一个边缘节点动作 $\boldsymbol{a}_{e}^{t}=\mu_{\mathbf{E}}\left(\boldsymbol{o}_{e}^{t},  \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{t} \mid \theta_{\mathbf{E}}^{\mu}\right)+\epsilon_{e} \mathcal{N}_{e}^{t}$\\
			接收奖励 $\boldsymbol{r}^{t}$ 和下一个系统状态 $\boldsymbol{o}^{t+1}$\\
			\For{\songti{车辆} $v=1$ \songti{到} $V$}{
				根据公式\ref{equ 4-40}计算车辆的差分奖励\\
			}
			根据公式\ref{equ 4-41}计算边缘节点的归一化奖励\\
			存储 $\left(\boldsymbol{o}^{t}, \boldsymbol{a}_{\mathbf{V}}^{t}, \boldsymbol{a}_{e}^{t}, \boldsymbol{r}_{\mathbf{V}}^{t}, \boldsymbol{r}_{e}^{t}, \boldsymbol{w}^{t}, \boldsymbol{o}^{t+1}\right)$ 到经验回放缓存 $\mathcal{B}$
		}
	}
\label{algorithm 4-2}
\end{algorithm}

\subsection{多智能体分布式策略执行}

在MAMO中，车辆和边缘节点分别通过本地策略网络分布式地决定动作。
车辆$v$在时间$t$上对系统状态的局部观测表示为：
	\begin{equation}
		\boldsymbol{o}_{v}^{t}=\left\{t, v, l_{v}^t, \mathbf{D}_{v}, \Phi_{v}, \mathbf{D}_{e}^{t}, \mathbf{D}_{\mathbf{I}_e^t}, \boldsymbol{w}^{t}\right\}
	\end{equation} 
\noindent 其中$t$为时间片索引；
$v$是车辆索引；$l_{v}^t$是车辆$v$的位置；
$\mathbf{D}_{v}$表示车辆$v$可以感知的信息集合；
$\Phi_{v}$代表$\mathbf{D}_{v}$中信息的感知开销；
$\mathbf{D}_{e}^{t}$ 代表$e$在时间$t$的边缘节点的缓存信息集；
$\mathbf{D}_{\mathbf{I}_e^t}$ 代表在时间$t$的边缘节点$e$中建模的视图所需的信息集合；
$\boldsymbol{w}^{t}$ 代表每个目标的权重向量，其在每次迭代中随机生成。
具体地，$\boldsymbol{w}^{t} = \begin{bmatrix}  w^{(1), t}  &  w^{(2), t} \end{bmatrix}$，其中$w^{(1), t} \in (0, 1)$和$w^{(2), t} \in (0, 1)$分别是VCPS质量和VCPS利润的权重，$\sum_{\forall i \in \{1, 2\}} w^{(j), t} = 1$。
另一方面，边缘节点$e$在时间$t$上对系统状态的局部观测表示为
\begin{equation}
	\boldsymbol{o}_{e}^{t}=\left\{t, e, \operatorname{\mathbf{Dis}}_{\mathbf{V}, e}^{t}, \mathbf{D}_{1}, \ldots, \mathbf{D}_{v}, \ldots, \mathbf{D}_{v}, \mathbf{D}_{e}^{t}, \mathbf{D}_{\mathbf{I}_e^t}, \boldsymbol{w}^{t} \right\}
\end{equation}
\noindent 其中$e$是边缘节点索引，$\operatorname{\mathbf{Dis}}_{\mathbf{V}, e}^{t}$代表车辆与边缘节点$e$之间的距离集合。
因此，系统在时间$t$的状态可以表示为$\boldsymbol{o}^{t}=\boldsymbol{o}_{e}^{t} \cup \boldsymbol{o}_{1}^{t} \cup \ldots \cup \boldsymbol{o}_{v}^{t} \cup \ldots \cup \boldsymbol{o}_{v}^{t}$。

车辆$v$的动作空间表示为：
\begin{equation}
	\boldsymbol{a}_{v}^{t} = \{ \mathbf{C}_v^t,  \{ \lambda_{d, v}^{t}, p_{d, v}^{t} \mid \forall d \in \mathbf{D}_{v}^t \} , \pi_v^t   \}
\end{equation}
其中，$\mathbf{C}_v^t$是感知决策；$\lambda_{d, v}^{t}$和$p_{d, v}^{t}$分别是信息$d$的感知频率和上传优先级，$\pi_v^t$是车辆$v$在时间$t$的传输功率。
车辆基于系统状态的本地观测，并通过本地车辆策略网络得到当前的动作。
\begin{equation}
	\boldsymbol{a}_{v}^{t}=\mu_{\mathbf{V}}\left(\boldsymbol{o}_{v}^{t} \mid \theta_{\mathbf{V}}^{\mu}\right)+\epsilon_{v} \mathcal{N}_{v}^{t}
\end{equation}
\noindent 其中，$\mathcal{N}_{v}^{t}$为探索噪音，以增加车辆动作的多样性，$\epsilon_{v}$为车辆$v$的探索常数。
车辆动作的集合被表示为 $\boldsymbol{a}_{\mathbf{V}}^{t} = \left\{\boldsymbol{a}_{v}^{t}\mid \forall v \in \mathbf{V}\right\}$。
另一方面，边缘节点$e$的动作空间表示为：
\begin{equation}
	\boldsymbol{a}_{e}^{t} = \{b_{v, e}^{t} \mid \forall v \in \mathbf{V}_{e}^{t}\}
\end{equation}
其中$b_{v, e}^t$是边缘节点$e$在时间$t$为车辆$v$分配的V2I带宽。
同样地，边缘节点$e$的动作可以由本地边缘策略网络根据系统状态以及车辆动作得到。
\begin{equation}
	\boldsymbol{a}_{e}^{t}=\mu_{\mathbf{E}}\left(\boldsymbol{o}_{e}^{t},  \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{t} \mid \theta_{\mathbf{E}}^{\mu}\right)+\epsilon_{e} \mathcal{N}_{e}^{t}
\end{equation}
\noindent 其中$\mathcal{N}_{e}^{t}$和$\epsilon_{e}$分别为边缘节点$e$的探索噪声和探索常数。
此外，车辆和边缘节点的联合动作被表示为 $\boldsymbol{a}^{t}= \left\{\boldsymbol{a}_{e}^{t}, \boldsymbol{a}_{1}^{t}, \ldots, \boldsymbol{a}_{v}^{t}, \ldots, \boldsymbol{a}_{V}^{t}\right\}$。

环境通过执行联合动作获得系统奖励向量，其表示为：
	\begin{equation}
	\boldsymbol{r}^{t} = \begin{bmatrix}  r^{(1)}\left(\boldsymbol{a}_{\mathbf{V}}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right)  &  r^{(2)}\left(\boldsymbol{a}_{\mathbf{V}}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right) \end{bmatrix} ^{T}
	\end{equation}
	\noindent 其中 $r^{(1)}\left(\boldsymbol{a}_{\mathbf{V}}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right)$ 和 $r^{(2)}\left(\boldsymbol{a}_{\mathbf{V}}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right)$ 分别是两个目标（即实现的VCPS质量和VCPS 利润）的奖励，可以通过下式计算：  
	\begin{numcases}{}
			r^{(1)}\left(\boldsymbol{a}_{\mathbf{V}}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right)=\frac{1}{\left|\mathbf{I}_e^t\right|} \sum_{\forall i \in \mathbf{I}_e^t}\operatorname{QV}_{i} \notag \\
			r^{(2)}\left(\boldsymbol{a}_{\mathbf{V}}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right)=\frac{1}{\left|\mathbf{I}_e^t\right|} \sum_{\forall i \in \mathbf{I}_e^t} \operatorname{PV}_{j} 
	\end{numcases}
因此，车辆$v$在第$i$个目标中的奖励可以通过基于差分奖励的信用分配方案 \cite{foerster2018counterfactual} 得到，其为系统奖励和没有其行动所取得的奖励之间的差值，其表示为：
\begin{equation}
r_{v}^{(j), t}=r^{(j)}\left(\boldsymbol{a}_{\mathbf{V}}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right)-r^{(j)}\left(\boldsymbol{a}_{\mathbf{V}-s}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right), \forall i \in \{1, 2\}
\label{equ 4-40}
\end{equation}
\noindent 其中 $r^{(j)}\left(\boldsymbol{a}_{\mathbf{V}-s}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right)$ 是在没有车辆$v$贡献的情况下实现的系统奖励，它可以通过设置车辆$v$的空动作集得到。
车辆$v$在时间$t$的奖励向量表示为$\boldsymbol{r}_{v}^{t} = \begin{bmatrix}  r_{v}^{(1), t}  &  r_{v}^{(2), t} \end{bmatrix} ^{T}$。
车辆的差分奖励集合表示为 $\boldsymbol{r}_{\mathbf{V}}^{t}=\{ \boldsymbol{r}_{v}^{t} \mid \forall v \in \mathbf{V}\}$。

另一方面，系统奖励通过最小-最大归一化进一步转化为边缘节点的归一化奖励。
边缘节点$e$在时间$t$的第$i$个目标中的奖励由以下方式计算：
\begin{equation}
	r_{e}^{(j), t}= \frac{r^{(j)}\left(\boldsymbol{a}_{\mathbf{V}}^{t},\boldsymbol{a}_{e}^{t} \mid \boldsymbol{o}^{t}\right) - \min \limits_{\forall {\boldsymbol{a}_{e}^{t}}^{\prime}} r^{(j)}\left(\boldsymbol{a}_{\mathbf{V}}^{t}, {\boldsymbol{a}_{e}^{t}}^{\prime} \mid \boldsymbol{o}^{t}\right)} {\max \limits_{\forall {\boldsymbol{a}_{e}^{t}}^{\prime}} r^{(j)}\left(\boldsymbol{a}_{\mathbf{V}}^{t}, {\boldsymbol{a}_{e}^{t}}^{\prime} \mid \boldsymbol{o}^{t}\right) - \min \limits_{\forall {\boldsymbol{a}_{e}^{t}}^{\prime}} r^{(j)}\left(\boldsymbol{a}_{\mathbf{V}}^{t}, {\boldsymbol{a}_{e}^{t}}^{\prime} \mid \boldsymbol{o}^{t}\right)}
\label{equ 4-41}
\end{equation}
\noindent 其中 $\min \limits_{\forall {\boldsymbol{a}_{e}^{t}}^{\prime}} r^{(j)} (\boldsymbol{a}_{\mathbf{V}}^{t}, {\boldsymbol{a}_{e}^{t}}^{\prime} \mid \boldsymbol{o}^{t})$ 和 $\max \limits_{\forall {\boldsymbol{a}_{e}^{t}}^{\prime}} r^{(j)}(\boldsymbol{a}_{\mathbf{V}}^{t}, {\boldsymbol{a}_{e}^{t}}^{\prime} \mid \boldsymbol{o}^{t})$ 分别是在相同的系统状态$\boldsymbol{o}^{t}$下，车辆动作$\boldsymbol{a}_{\mathbf{V}}^{t}$不变时，可实现的系统奖励最小值和最大值。
边缘节点$e$在时间$t$的奖励向量表示为 $\boldsymbol{r}_{e}^{t} = \begin{bmatrix}  r_{e}^{(1), t}  &  r_{e}^{(2), t} \end{bmatrix} ^{T}$。
交互经验包括当前系统状态$\boldsymbol{o}^{t}$、车辆动作$\boldsymbol{a}_{\mathbf{V}}^{t}$、边缘节点动作$\boldsymbol{a}_{e}^{t}$、车辆奖励$\boldsymbol{r}_{\mathbf{V}}^{t}$、边缘节点奖励$\boldsymbol{r}_{e}^{t}$、权重$\boldsymbol{w}^{t}$，以及下一时刻系统状态$\boldsymbol{o}^{t+1}$都存储到经验回放缓存$\mathcal{B}$。

\subsection{多目标策略评估}

本章节阐述了针对多目标的策略评估，具体地，提出了决斗评论家网络，根据状态的价值和行动的优势来评估智能体的行动。
在DCN中有两个全连接的网络，即动作优势网络和状态价值网络。
车辆和边缘节点的AA网络参数分别表示为 $\theta_{\mathbf{V}}^{\mathscr{A}}$ 和 $\theta_{\mathbf{E}}^{\mathscr{A}}$。
同样，车辆和边缘节点的SV网络的参数分别表示为 $\theta_{\mathbf{V}}^{\mathscr{V}}$ 和 $\theta_{\mathbf{E}}^{\mathscr{V}}$。
用$A_{\mathbf{V}}\left({o}_{v}^{m},  {a}_{v}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{V}}^{\mathscr{A}} \right)$表示车辆$v$中AA网络的输出标量, 其中 $\boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m}$ 表示其他车辆动作。
同样地，以边缘节点$e$为输入的AA网络的输出标量表示为 $A_{\mathbf{E}}\left({o}_{e}^{m},  {a}_{e}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{E}}^{\mathscr{A}} \right)$, 其中 $\boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m}$ 表示所有车辆动作。
车辆$v$的SV网络的输出标量表示为 $V_{\mathbf{V}}\left({o}_{v}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{V}}^{\mathscr{V}} \right)$。
同样地，边缘节点$e$的SV网络的输出标量表示为 $V_{\mathbf{E}}\left({o}_{e}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{E}}^{\mathscr{V}} \right)$。

多目标策略评估由三个步骤组成。
首先，AA网络基于观测、动作和权重输出智能体动作的优势。
其次，VS网络根据观测和权重，输出当前状态的价值。
最后，采用一个聚合模块，根据行动优势和状态价值，输出智能体动作的价值。
具体来说，在AA网络中随机生成$N$个动作并将智能体动作替换，以评估当前动作对于随机行动的平均优势。
用${a}_{v}^{m, n}$和${a}_{e}^{m, n}$分别表示车辆$v$和边缘节点$e$的第$n$个随机动作。
因此，车辆$v$和边缘节点$e$的第$n$个随机动作的优势可分别表示为 $A_{\mathbf{V}}\left({o}_{v}^{m},  {a}_{v}^{m, n}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m}, \boldsymbol{w}^{m} \mid \theta_{v}^{\mathscr{A}} \right)$ 和 $A_{\mathbf{E}}\left({o}_{e}^{m},  {a}_{e}^{m, n}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{E}}^{\mathscr{A}} \right)$。

进一步，通过评估智能体动作相对于随机动作的平均优势，对价值函数进行聚合。
因此，车辆$v\in\mathbf{V}$和边缘节点$e$的动作价值是通过下式计算： 
\begin{align}
    Q_{\mathbf{V}}\left({o}_{v}^{m}, {a}_{v}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{V}}^{Q} \right) &= V_{\mathbf{V}}\left({o}_{v}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{V}}^{\mathscr{V}} \right) + A_{\mathbf{V}}\left({o}_{v}^{m},  {a}_{v}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{V}}^{\mathscr{A}} \right) \notag \\
    &- \frac{1}{N} \sum_{\forall n} A_{\mathbf{V}}\left({o}_{v}^{m},  {a}_{v}^{m, n}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{V}}^{\mathscr{A}} \right)
\end{align}
\begin{align}
    Q_{E}\left({o}_{e}^{m},  {a}_{e}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{E}}^{Q} \right) &= V_{\mathbf{E}}\left({o}_{e}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{E}}^{\mathscr{V}} \right) + A_{\mathbf{E}}\left({o}_{e}^{m},  {a}_{e}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{E}}^{\mathscr{A}} \right) \notag \\
    &- \frac{1}{N} \sum_{\forall n} A_{\mathbf{E}}\left({o}_{e}^{m},  {a}_{e}^{m, n}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{E}}^{\mathscr{A}} \right)
\end{align}
其中，$\theta_{\mathbf{V}}^{Q}$ 和 $\theta_{\mathbf{V}}^{Q}$ 包含相应的AA和SV网络的参数。
\begin{align}
	\theta_{\mathbf{V}}^{Q} = (\theta_{\mathbf{V}}^{\mathscr{A}}, \theta_{\mathbf{V}}^{\mathscr{V}}), \theta_{\mathbf{V}}^{Q^{\prime}} = (\theta_{\mathbf{V}}^{\mathscr{A}^{\prime}}, \theta_{\mathbf{V}}^{\mathscr{V}^{\prime}}) \\
	\theta_{\mathbf{E}}^{Q} = (\theta_{\mathbf{E}}^{\mathscr{A}}, \theta_{\mathbf{E}}^{\mathscr{V}}), \theta_{\mathbf{E}}^{Q^{\prime}} = (\theta_{\mathbf{E}}^{\mathscr{A}^{\prime}}, \theta_{\mathbf{E}}^{\mathscr{V}^{\prime}})
\end{align}

\subsection{网络学习和更新}

从经验回放缓存$\mathcal{B}$中抽出$M$小批量，以训练车辆和边缘节点的策略和评论家网络，其中单个样本表示为 $\left(\boldsymbol{o}_{\mathbf{V}}^{m}, {o}_{e}^{m}, \boldsymbol{w}^{m}, \boldsymbol{a}_{\mathbf{V}}^{m}, {a}_{e}^{m}, \boldsymbol{r}_{\mathbf{V}}^{m}, \boldsymbol{r}_{e}^{m}, \boldsymbol{o}_{\mathbf{V}}^{m+1}, {o}_{e}^{m+1}, \boldsymbol{w}^{m+1}\right)$。
车辆$v$的目标值表示为：
\begin{equation}
	y_{v}^{m} = \boldsymbol{r}_{v}^{m} \boldsymbol{w}^{m} +\gamma Q_{\mathbf{V}}^{\prime}\left({o}_{v}^{m+1},  {a}_{v}^{m+1}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m+1}, \boldsymbol{w}^{m+1} \mid \theta_{\mathbf{V}}^{Q^{\prime}} \right)
\end{equation}
\noindent 其中 $Q_{\mathbf{V}}^{\prime}({o}_{v}^{m+1},  {a}_{v}^{m+1}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m+1}, \boldsymbol{w}^{m+1} \mid \theta_{\mathbf{V}}^{Q^{\prime}})$ 是目标车辆评论家网络产生的动作价值。
$\gamma$是折扣因子。
$\boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m+1}$ 是没有车辆$v$的下一时刻车辆动作。
\begin{equation}
	\boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m+1} = \{ {a}_{1}^{m+1}, \ldots, {a}_{s-1}^{m+1}, {a}_{s+1}^{m+1}, \ldots, {a}_{v}^{m+1} \}
\end{equation}
而 ${a}_{v}^{m+1}$ 是目标车辆策略网络根据对下一时刻系统状态的局部观测产生的车辆$v$的下一时刻动作。
\begin{equation}
	{a}_{v}^{m+1} = \mu_{\mathbf{V}}^{\prime}(\boldsymbol{o}_{v}^{m+1} \mid \theta_{\mathbf{V}}^{\mu^{\prime}})
\end{equation}
类似地，边缘节点$e$的目标值表示为：
\begin{equation}
	y_{e}^{m} = \boldsymbol{r}_{e}^{m} \boldsymbol{w}^{m} +\gamma Q_{\mathbf{E}}^{\prime}\left({o}_{e}^{m+1},  {a}_{e}^{m+1}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m+1}, \boldsymbol{w}^{m+1} \mid \theta_{\mathbf{E}}^{Q^{\prime}} \right)
\end{equation}
\noindent 其中 $Q_{\mathbf{E}}^{\prime}({o}_{e}^{m+1},  {a}_{e}^{m+1}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m+1}, \boldsymbol{w}^{m+1} \mid \theta_{\mathbf{E}}^{Q^{\prime}})$ 表示由目标边缘评论家网络产生的动作价值。
$\boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m+1}$ 是下一时刻车辆动作。
${a}_{e}^{m+1}$表示下一时刻边缘节点动作，该动作可由目标边缘策略网络根据其对下一时刻系统状态的局部观测获得，即${a}_{e}^{m+1} = \mu_{\mathbf{E}}^{\prime}(\boldsymbol{o}_{e}^{m+1}, \boldsymbol{a}_{\mathbf{V}}^{m+1} \mid \theta_{\mathbf{E}}^{\mu^{\prime}})$。

车辆评论家网络和边缘评论家网络的损失函数是通过分类分布的时间差分（Temporal Difference, TD）学习得到的，其表示为：
\begin{equation}
	\mathcal{L}\left(\theta_{\mathbf{V}}^{Q}\right)=\frac{1}{M} \sum_{m} \frac{1}{S} \sum_{v} {Y_v^{m}}
\end{equation}
\begin{equation}
	\mathcal{L}\left(\theta_{\mathbf{E}}^{Q}\right)=\frac{1}{M} \sum_{m} {Y_e^{m}}
\end{equation}
\noindent 其中$Y_v^{m}$和$Y_e^{m}$分别是车辆$v$和边缘节点$e$的目标值和局部评论家网络产生的动作价值之差的平方。
\begin{equation}
	\begin{aligned}
		Y_v^{m} &= \left(y_{v}^{m}-Q_{\mathbf{V}}\left({o}_{v}^{m},  {a}_{v}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{V}}^{Q} \right)\right)^{2} \\
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		Y_e^{m} &=\left(y_{e}^{m}-Q_{\mathbf{E}}\left({o}_{e}^{m},  {a}_{e}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{V}}^{Q} \right)\right)^{2} \\
	\end{aligned}
\end{equation}
车辆和边缘策略网络参数通过确定性的策略梯度进行更新。
\begin{equation}
	\nabla_{\theta_{\mathbf{V}}^{\mu}} \mathcal{J} (\theta_{\mathbf{V}}^{\mu}) \approx \frac{1}{M} \sum_{m} \frac{1}{S} \sum_{v} P_{v}^{m} 
\end{equation}
\begin{equation}
	\nabla_{\theta_{\mathbf{E}}^{\mu}} \mathcal{J} (\theta_{\mathbf{E}}^{\mu}) \approx \frac{1}{M} \sum_{m} P_{e}^{m} 
\end{equation}
\noindent 其中 
\begin{equation}
P_{v}^{m} = \nabla_{{a}_{v}^{m}} Q_{\mathbf{V}}\left({o}_{v}^{m}, {a}_{v}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}-v}^{m}, \boldsymbol{w}^{m} \mid \theta_{v}^{Q} \right) \nabla_{\theta_{\mathbf{V}}^{\mu}} \mu_{\mathbf{V}}\left({o}_{v}^{m} \mid \theta_{\mathbf{V}}^{\mu}\right)
\end{equation}
\begin{equation}
P_{e}^{m} = \nabla_{{a}_{e}^{m}} Q_{\mathbf{E}}\left({o}_{e}^{m}, {a}_{e}^{m}, \boldsymbol{a}_{\boldsymbol{\mathbf{V}}}^{m}, \boldsymbol{w}^{m} \mid \theta_{\mathbf{E}}^{Q} \right) \nabla_{\theta_{\mathbf{E}}^{\mu}} \mu_{\mathbf{E}}\left({o}_{e}^{m}, {\boldsymbol{a}}_{\boldsymbol{\mathbf{V}}}^{m} \mid \theta_{\mathbf{E}}^{\mu}\right)
\end{equation}

本地策略和评论家网络参数分别以$\alpha$和$\beta$的学习率更新。
特别地，车辆和边缘节点定期更新目标网络的参数，即当$t \mod t_{\operatorname{tgt}} = 0$, 其中 $t_{\operatorname{tgt}}$ 是目标网络的参数更新周期。
\begin{align}
	\theta_{\mathbf{V}}^{\mu^{\prime}} \leftarrow n_{\mathbf{V}} \theta_{\mathbf{V}}^{\mu}+(1-n_{\mathbf{V}}) \theta_{\mathbf{V}}^{\mu^{\prime}}, \theta_{\mathbf{V}}^{Q^{\prime}} \leftarrow n_{\mathbf{V}} \theta_{\mathbf{V}}^{Q}+(1-n_{\mathbf{V}}) \theta_{\mathbf{V}}^{Q^{\prime}}\\
	\theta_{\mathbf{E}}^{\mu^{\prime}} \leftarrow n_{\mathbf{E}} \theta_{\mathbf{E}}^{\mu}+(1-n_{\mathbf{E}}) \theta_{\mathbf{E}}^{\mu^{\prime}}, \theta_{\mathbf{E}}^{Q^{\prime}} \leftarrow n_{\mathbf{E}} \theta_{\mathbf{E}}^{Q}+(1-n_{\mathbf{E}})  \theta_{\mathbf{E}}^{Q^{\prime}}
\end{align}
\noindent 其中 $n_{\mathbf{V}} \ll 1$ 和 $n_{\mathbf{E}} \ll 1$。
同样，分布式行动者的策略网络参数也会定期更新，即当$t \mod t_{\operatorname{act}} = 0$，其中 $t_{\operatorname{act}}$ 是分布式行动者的策略网络的参数更新周期。
\begin{align}
	\theta_{\mathbf{V}, k}^{\mu} \leftarrow \theta^{{\mu}^{\prime}}_{\mathbf{V}}, \theta_{\mathbf{V}, k}^{Q} \leftarrow \theta_{\mathbf{V}}^{Q^{\prime}}, \forall k \in \{1, 2, \ldots, K\}\\
	\theta_{\mathbf{E}, k}^{\mu} \leftarrow \theta_{\mathbf{E}}^{\mu^{\prime}}, \theta_{\mathbf{E}, k}^{Q} \leftarrow \theta_{\mathbf{E}}^{Q^{\prime}}, \forall k \in \{1, 2, \ldots, K\}
\end{align}

\section{实验设置与结果分析}\label{section 4-6}

\subsection{实验设置}

本章节使用Python 3.9.13和TensorFlow 2.8.0来搭建仿真实验模型以评估所提MAMO方案的性能，其运行在配备AMD Ryzen 9 5950X 16核处理器@ 3.4 GHz，两个NVIDIA GeForce RTX 3090 GPU和64 GB内存的Ubuntu 20.04服务器上。
在参考\inlinecite{sadek2009distributed}和\inlinecite{wang2019delay}的基础上，实验仿真参数设置如下：
V2I通信范围被设定为500 m。
传输功率被设定为100 mW。
AWGN和可靠性阈值分别设置为-90 dBm和0.9。
V2I通信的信道衰减增益遵循高斯分布，其均值为2，方差为0.4。
$\hat{\Theta_{i}}$、$\hat{\Psi_{i}}$、$\hat{\Xi_{i}}$、$\hat{\Phi_{i}}$和$\hat{\Omega_{i}}$的加权系数分别设置为0.6、0.4、0.2、0.4和0.4。

MAMO中策略和评论家网络的架构和超参数描述如下：
本地策略网络是一个有两个隐藏层的四层全连接神经网络，其中神经元的数量分别为256和128。
目标策略网络的结构与本地策略网络相同。
本地评论家网络是一个四层全连接神经网络，有两个隐藏层，其中神经元的数量分别为512和256。
目标评论家网络的结构与本地评论家网络相同。
折扣率、批大小和最大经验回放缓存大小分别为0.996、256和1$\times10^{6}$。
策略网络和评论家网络的学习率分别为1$\times10^{-4}$和1$\times10^{-4}$。

进一步，本章节实现了三个比较算法，其具体细节介绍如下：
\begin{itemize}
	\item \textbf{随机分配}: 随机选择一个动作来确定感知信息、感知频率、上传优先级、传输功率和V2I带宽分配。
	\item \textbf{分布式深度确定性策略梯度}\cite{barth2018distributed}: 在边缘节点实现了一个智能体，根据系统状态，集中式地确定感知信息、感知频率、上传优先级、传输功率和V2I带宽分配。VCPS质量和VCPS 利润权重分别设定为0.5和0.5。
	\item \textbf{多智能体分布式深度确定性策略梯度}: 其为D4PG的多智能体版本，并在车辆上分布式实现，根据对物理环境的局部观测决定感知信息、感知频率、上传优先级和传输功率，边缘节点决定V2I带宽分配。VCPS质量和VCPS 利润权重分别设为0.5和0.5。
\end{itemize}

为了评估算法在视图建模质量和有效性方面的表现，本章设计了以下两个新的指标。
\begin{itemize}
	\item \textbf{单位开销质量}：其定义为花费单位开销实现的VCPS质量，其计算公式为：
		\begin{equation}
			\operatorname{QPUC}=\frac{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} \sum_{\forall i \in \mathbf{I}_e^t} \mathrm{QV}_i}{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} \sum_{\forall i \in \mathbf{I}_e^t} \mathrm{CV}_i}
		\end{equation}
		其中$\mathrm{QV}_i$和$\mathrm{CV}_i$分别是视图$i$的质量和开销。
	\item \textbf{单位质量利润}：其定义为单位VCPS质量所实现的VCPS 利润，其计算公式为：
		\begin{equation}
		\operatorname{PPUQ}=\frac{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} \sum_{\forall i \in \mathbf{I}_e^t}\mathrm{PV}_i}{\sum_{\forall t \in \mathbf{T}} \sum_{\forall e \in \mathbf{E}} \sum_{\forall i \in \mathbf{I}_e^t} \mathrm{QV}_i}
		\end{equation}
		其中$\mathrm{PV}_i$和$\mathrm{CV}_i$分别是视图$i$的利润和开销。
\end{itemize}
QPUC越高表明它能在相同的开销下实现更高的VCPS质量，而PPUQ越高表明它能更有效地使用感知和通信资源。上述指标全面显示了算法在同时最大化VCPS质量和最小化VCPS 开销的性能。
本章进一步基于公式\ref{equ 4-16}、\ref{equ 4-20}、\ref{equ 4-21}和\ref{equ 4-23}设计了四个指标，分别是\textbf{平均及时性}（Average Timeliness, AT）、\textbf{平均冗余度}（Average Redundancy, AR）、\textbf{平均感知开销}（Average Sensing Cost, ASC）和\textbf{平均传输开销}（Average Transmission Cost, ATC）。 

\subsection{实验结果与分析}

\textbf{1) 算法收敛性：}图\ref{fig 4-3}比较了四种算法的收敛性。其中，图\ref{fig 4-3}(a)和\ref{fig 4-3}(b)分别展示了四种算法的QPUC和PPUQ表现。X轴表示迭代次数，Y轴表示达到的QPUC和PPUQ。QPUC和PPUQ越高，表明算法在VCPS质量和VCPS开销方面表现越好。MAMO在大约850次迭代后，达到了最高的QPUC（约13.6）和最高的PPUQ（约1.13）。相比之下，RA、D4PG和MAD4PG分别实现了约2.29、7.34和2.58的QPUC，并分别实现了约0.87、0.99和0.81的PPUQ。与RA、D4PG和MAD4PG相比，MAMO在QPUC方面分别实现了约494.1\%、85.5\%和428.8\%的提升，在PPUQ方面分别实现了约30.6\%、14.2\%和40.7\%的改善。值得注意的是，MAMO是唯一一个能够同时改善QPUC和PPUQ的方案。这显示了MAMO在同时实现QPUC和PPUQ最大化方面的优势。

\begin{figure}[h]
 \centering
 \includegraphics[width=1\columnwidth]{Fig4-3-different-algorithms.pdf}
 \bicaption[算法收敛性比较]{算法收敛性比较，其显示与RA、D4PG和MAD4PG相比，MAMO在收敛后（约850次迭代）达到了最高的QPUC和最高的PPUQ。（a）单位开销质量（b）单位质量利润}[Convergence comparison]{Convergence comparison, which shows MAMO achieves the highest QPUC and the highest PPUQ compared with RA, D4PG, and MAD4PG after convergence (around 850 iterations). (a) Quality per unit cost (b) Profit per unit quality}
 \label{fig 4-3}
\end{figure}

\begin{figure}[h]
 \centering
 \includegraphics[width=1\columnwidth]{Fig4-4-different-networks.pdf}
 \bicaption[隐藏层中不同数量神经元下MAMO性能比较]{隐藏层中不同数量神经元下MAMO性能比较。（a）单位开销质量（b）单位质量利润}[Performance comparison of MAMO under different numbers of neurons in the hidden layers]{Performance comparison of MAMO under different numbers of neurons in the hidden layers. (a) Quality per unit cost (b) Profit per unit quality}
 \label{fig 4-4}
\end{figure}

\textbf{2) 神经元数量的影响：}
图\ref{fig 4-4}比较了不同神经元数量下MAMO的性能。其中，X轴表示策略网络和评论家网络的两个隐藏层的神经元数量，分别设置为[64, 32] $\sim$ [1024, 512]和[128, 64] $\sim$ [2048, 1024]。如图\ref{fig 4-4}(a)所示，当策略网络和评论家网络的隐藏层的神经元数量设置为默认值（即[256, 128]和[512, 256]）时，MAMO实现了最高的VCPS质量和利润。图\ref{fig 4-4}(b)比较了其他三个指标，包括AT、ASC和ATC。AT、ASC和ATC越低，说明在信息新鲜度、感知开销和传输开销方面表现越好。可以注意到，当每个隐藏层的神经元数量为默认设置时，MAMO在最小化AT、ASC和ATC方面表现最佳。

\begin{figure}[h]
 \centering
 \includegraphics[width=1\columnwidth]{Fig4-5-different-scenarios.pdf}
 \bicaption[不同交通场景下的性能比较]{不同交通场景下的性能比较。（a）单位开销质量（b）单位质量利润（c）平均感知开销（d）平均传输开销}[Performance comparison under different traffic scenarios]{Performance comparison under different traffic scenarios. (a) Quality per unit cost (b) Profit per unit quality (c) Average sensing cost (d) Average transmission cost}
 \label{fig 4-5}
\end{figure}

\textbf{3) 交通情况的影响：}
图\ref{fig 4-5}比较了四种算法在不同交通场景下的性能。X轴表示交通场景，不同场景在不同的时间和空间中提取了现实的车辆轨迹作为输入，分别为：1）2016年11月16日8:00至8:05，中国成都市青羊区1平方公里区域；2）同日23:00至23:05，同一区域；3）2016年11月27日8:00至8:05，中国西安碑林区1平方公里区域。图\ref{fig 4-5}(a)比较了四种算法的QPUC，MAMO在所有场景下都取得了最高的QPUC。图\ref{fig 4-5}(b)比较了四种算法的PPUQ，MAMO在所有情况下都表现最好。与RA、D4PG和MAD4PG相比，MAMO分别提高了589.0\%、106.7\%和514.8\%的QPUC，并分别提高了约41.6\%、23.6\%和45.7\%的PPUQ。图\ref{fig 4-5}(c)比较了四种算法的ASC。MAMO的ASC低于RA、D4PG和MAD4PG，说明MAMO可以实现车辆协同感知以降低感知开销。图\ref{fig 4-5}(d)比较了四种算法的ATC，在不同情况下，MAMO的ATC最低。

\begin{figure}[h]
 \centering
 \includegraphics[width=1\columnwidth]{Fig4-6-different-bandwidths.pdf}
 \bicaption[不同V2I带宽下的性能比较]{不同V2I带宽下的性能比较。（a）单位开销质量（b）单位质量利润（c）平均及时性（d）平均冗余度（e）平均感知开销（f）平均传输开销}[Performance comparison under different V2I bandwidths]{Performance comparison under different V2I bandwidths. (a) Quality per unit cost (b) Profit per unit quality (c) Average timeliness (d) Average redundancy (e) Average sensing cost (f) Average transmission cost}
 \label{fig 4-6}
\end{figure}

\begin{figure}[h]
 \centering
 \includegraphics[width=1\columnwidth]{Fig4-7-different-numbers.pdf}
 \bicaption[不同视图需求下的性能比较]{不同视图需求下的性能比较。（a）单位开销质量（b）单位质量利润（c）平均及时性（d）平均冗余度（e）平均感知开销（f）平均传输开销}[Performance comparison under different digit twin requirements]{Performance comparison under different digit twin requirements. (a) Quality per unit cost (b) Profit per unit quality (c) Average timeliness (d) Average redundancy (e) Average sensing cost (f) Average transmission cost}
 \label{fig 4-7}
\end{figure}

\textbf{4) V2I带宽的影响：}
图\ref{fig 4-6}比较了四种算法在不同V2I带宽下的性能。X轴表示V2I带宽，从1MHz增加到3MHz。较大的V2I带宽代表每辆车被分配的V2I带宽也随之增加。图\ref{fig 4-6}(a)比较了四种算法的QPUC。随着带宽的增加，MAMO的QPUC也相应增加。这是因为在带宽富余的场景中，MAMO中车辆之间的协同感知和上传更加有效。图\ref{fig 4-6}(b)显示了四种算法的PPUQ，可以进一步证明这一优势。如图\ref{fig 4-6}(b)所示，MAMO在不同的V2I带宽下实现了最高的PPUQ。特别地，与RA、D4PG和MAD4PG相比，MAMO分别提高了约453.3\%、131.4\%和437.6\%的QPUC，并使PPUQ提高了约33.0\%、18.3\%和48.4\%。图\ref{fig 4-6}(c)比较了四种算法的AT，MAMO实现了最低的AT。当带宽从2.5MHz增加到3MHz时，MAMO和D4PG的性能差距很小。这是因为随着带宽的增加，视图的及时性改善是有限的。图\ref{fig 4-6}(d)比较了四种算法的AR。AR越低意味着协同感知和上传的性能越好，MAMO实现了最低的AR。图\ref{fig 4-6}(e)和\ref{fig 4-6}(f)分别比较了四种算法的ASC和ATC。可以看出，当带宽增加时，这四种算法的ATC都会下降。原因是，当带宽增加时，信息上传时间减少，导致传输开销降低。MAMO的ASC和ATC在大多数情况下保持在最低水平。

\textbf{5) 视图需求的影响：}
图\ref{fig 4-7}比较了四种算法在不同视图需求下的性能，其中X轴表示视图所需信息的平均数量从3增加到7。视图所需信息的平均数越大，说明车辆的感知和上传工作负荷越大。图\ref{fig 4-7}(a)比较了四种算法的QPUC。随着平均所需信息数的增加，四种算法的QPUC也相应减少。然而，MAMO在所有情况下保持最高的QPUC。图\ref{fig 4-7}(b)比较了四种算法的PPUQ。正如预期的那样，MAMO在所有情况下都取得了最高的PPUQ。特别地，与RA、D4PG和MAD4PG相比，MAMO的QPUC分别高出458.7\%、130.6\%和426.2\%，PPUQ分别高出31.5\%、18.2\%和40.7\%。图\ref{fig 4-7}(c)比较了四种算法的AT。MAMO在AT方面取得了最佳性能。图\ref{fig 4-7}(d)比较了四种算法的AR，表明MAMO可以在所有情况下实现最低的AR。图\ref{fig 4-7}(e)和\ref{fig 4-7}(f)分别比较了四种算法的ASC和ATC。值得注意的是，当平均信息数增加时，四种算法的ASC和ATC都会增加。原因是视图需要的平均信息量增加，导致车辆感应和传输开销提高。

\section{本章小结}\label{section 4-7}

本章提出了协同感知与V2I上传场景，其中基于车辆协同感知与V2I协同上传构建逻辑视图。
具体地，基于多类M/G/1优先级队列构建了协同感知模型，并基于信道衰减分布和SNR阈值构建了V2I协同上传模型。
在此基础上，设计了两个指标QV和CV，以衡量在边缘节点建模的视图的质量和开销，并形式化定义了一个双目标优化问题，通过协同感知和上传，最大化VCPS质量的同时，最小化VCPS 开销。
进一步，提出了一个基于多目标的多智能体深度强化学习算法，其中采用了一个决斗评论家网络，根据状态价值和动作优势来评估智能体动作。
最后，进行了全面的性能评估，证明了所提MAMO算法的优越性。